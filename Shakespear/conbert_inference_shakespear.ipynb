{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGe7n5xR3gMT"
      },
      "source": [
        "This notebook reproduces creation of CondBERT vocabulary. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/s-nlp/detox"
      ],
      "metadata": {
        "id": "JA_bnXWsGzCs",
        "outputId": "abd69fad-e35f-4c0e-a67e-d9677046147b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'detox'...\n",
            "remote: Enumerating objects: 180, done.\u001b[K\n",
            "remote: Counting objects: 100% (180/180), done.\u001b[K\n",
            "remote: Compressing objects: 100% (158/158), done.\u001b[K\n",
            "remote: Total 180 (delta 50), reused 118 (delta 14), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (180/180), 17.92 MiB | 12.89 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r detox/requirements.txt -q"
      ],
      "metadata": {
        "id": "_bzxtQqsG3Hh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bfc3bba-8b62-4a6b-cc16-d78539c2ce67"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.8/400.8 KB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.5/788.5 KB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 KB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.6/269.6 KB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 KB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 KB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for fairseq \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for fairseq\u001b[0m\u001b[31m\n",
            "\u001b[0m  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not build wheels for fairseq, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "parHQpkf3gMV"
      },
      "source": [
        "# 0. Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "def add_sys_path(p):\n",
        "    p = os.path.abspath(p)\n",
        "    print(p)\n",
        "    if p not in sys.path:\n",
        "        sys.path.append(p)"
      ],
      "metadata": {
        "id": "qZxKA3CC3vhL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_sys_path('detox/emnlp2021/style_transfer/condBERT')"
      ],
      "metadata": {
        "id": "lgIlDsUHIiT5",
        "outputId": "bee33aaf-a631-4faa-ef4d-7acbccb10172",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/detox/emnlp2021/style_transfer/condBERT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib import reload\n",
        "import condbert\n",
        "reload(condbert)\n",
        "from condbert import CondBertRewriter"
      ],
      "metadata": {
        "id": "2Kcp5gluIlqj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgrasmvn3gMW"
      },
      "source": [
        "# 1. Loading BERT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "dzEg-P_I5ZmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "361ea559-fa00-42e7-ee81-22ee00320618"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.2-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Using cached huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CMMVz8xs3gMX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm.auto import tqdm, trange\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0')\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForMaskedLM.from_pretrained(model_name)\n",
        "model.to(device);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fjDeZkQeJzJ",
        "outputId": "0e10f2b7-693b-4a16-e874-e546a3dc0ed6"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_JLgGPj3gMY"
      },
      "source": [
        "# 2. Preparing the vocabularires."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMuIbhBw3gMY"
      },
      "source": [
        "\n",
        "- old-words.txt\n",
        "- modern-words.txt\n",
        "- word2coef.pkl\n",
        "- token_shakespearean.txt\n",
        "\n",
        "These files should be prepared once. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('shakespearen.csv')"
      ],
      "metadata": {
        "id": "aPLVNbRAN-up"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используемый датасет параллельный, поэтому мы возьмем разные части датасета для разных стилей."
      ],
      "metadata": {
        "id": "7-ZifEe-PgFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "In7OTe1xTQkC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  for a in '.,?!:;-)(':\n",
        "    text = text.replace(a, ' '+a)\n",
        "  text = text.replace('\"', '')\n",
        "  text = text.replace(\"'\", \"\")\n",
        "  text = re.sub(' +', ' ', text)\n",
        "  text = text.lower()\n",
        "  return text"
      ],
      "metadata": {
        "id": "H_JoVVirSKYA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['og'] = dataset['og'].apply(preprocess_text)\n",
        "dataset['t'] = dataset['t'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "jn67ap4rOc1h"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "op6LfmPcXCkQ",
        "outputId": "d92a9b3c-c423-46b4-8661-6139ff6fd9f7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Unnamed: 0                         id  \\\n",
              "0               0  42928-1500614319216-63344   \n",
              "1               1  42928-1500614326583-89821   \n",
              "2               2                    A-63849   \n",
              "3               3  42930-1500614347266-80123   \n",
              "4               4  42930-1500614355280-38326   \n",
              "...           ...                        ...   \n",
              "51782         205  40471-1502742711264-62106   \n",
              "51783         206  40471-1502742725153-49869   \n",
              "51784         207  40471-1502742735040-34786   \n",
              "51785         208  40471-1502742744235-12820   \n",
              "51786         209                    A-78849   \n",
              "\n",
              "                                                      og  \\\n",
              "0                    you do not meet a man but frowns :    \n",
              "1       our bloods no more obey the heavens than our ...   \n",
              "2                                but whats the matter ?    \n",
              "3       his daughter , and the heir ofs kingdom , who...   \n",
              "4       shes wedded ; her husband banishd ; she impri...   \n",
              "...                                                  ...   \n",
              "51782          he hath not told us of the captain yet .    \n",
              "51783   when that is known and golden time convents ,...   \n",
              "51784   meantime , sweet sister , we will not part fr...   \n",
              "51785   cesario , come , for so you shall be , while ...   \n",
              "51786  when that i was and a little tiny boy , with h...   \n",
              "\n",
              "                                                       t  \n",
              "0            every man you meet these days is frowning .  \n",
              "1       our bodies are in agreement with the planetar...  \n",
              "2                                          whats wrong ?  \n",
              "3      the king wanted his daughter , the only heir t...  \n",
              "4       shes married , her husband is banished , shes...  \n",
              "...                                                  ...  \n",
              "51782         he hasnt told us about that captain yet .   \n",
              "51783  when thats taken care of and the time is conve...  \n",
              "51784  until then , sweet sister -in -law , we wont l...  \n",
              "51785  cesario , come here . youll be cesario to me w...  \n",
              "51786  when i was just a tiny little boy ,with hey , ...  \n",
              "\n",
              "[51787 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ecc1bb5f-580b-4dac-9f67-4b433c7dd163\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>og</th>\n",
              "      <th>t</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>42928-1500614319216-63344</td>\n",
              "      <td>you do not meet a man but frowns :</td>\n",
              "      <td>every man you meet these days is frowning .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>42928-1500614326583-89821</td>\n",
              "      <td>our bloods no more obey the heavens than our ...</td>\n",
              "      <td>our bodies are in agreement with the planetar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>A-63849</td>\n",
              "      <td>but whats the matter ?</td>\n",
              "      <td>whats wrong ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>42930-1500614347266-80123</td>\n",
              "      <td>his daughter , and the heir ofs kingdom , who...</td>\n",
              "      <td>the king wanted his daughter , the only heir t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>42930-1500614355280-38326</td>\n",
              "      <td>shes wedded ; her husband banishd ; she impri...</td>\n",
              "      <td>shes married , her husband is banished , shes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51782</th>\n",
              "      <td>205</td>\n",
              "      <td>40471-1502742711264-62106</td>\n",
              "      <td>he hath not told us of the captain yet .</td>\n",
              "      <td>he hasnt told us about that captain yet .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51783</th>\n",
              "      <td>206</td>\n",
              "      <td>40471-1502742725153-49869</td>\n",
              "      <td>when that is known and golden time convents ,...</td>\n",
              "      <td>when thats taken care of and the time is conve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51784</th>\n",
              "      <td>207</td>\n",
              "      <td>40471-1502742735040-34786</td>\n",
              "      <td>meantime , sweet sister , we will not part fr...</td>\n",
              "      <td>until then , sweet sister -in -law , we wont l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51785</th>\n",
              "      <td>208</td>\n",
              "      <td>40471-1502742744235-12820</td>\n",
              "      <td>cesario , come , for so you shall be , while ...</td>\n",
              "      <td>cesario , come here . youll be cesario to me w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51786</th>\n",
              "      <td>209</td>\n",
              "      <td>A-78849</td>\n",
              "      <td>when that i was and a little tiny boy , with h...</td>\n",
              "      <td>when i was just a tiny little boy ,with hey , ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>51787 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ecc1bb5f-580b-4dac-9f67-4b433c7dd163')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ecc1bb5f-580b-4dac-9f67-4b433c7dd163 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ecc1bb5f-580b-4dac-9f67-4b433c7dd163');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "aqUGd1iI3gMZ"
      },
      "outputs": [],
      "source": [
        "old_corpus_path = dataset['og'][:20000].to_list()\n",
        "modern_corpus_path = dataset['t'][20000:40000].to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU736Fnq3gMZ"
      },
      "source": [
        "### 2.1 Preparing the DRG-like vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "TZV6NqWB3gMZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from nltk import ngrams\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "\n",
        "\n",
        "class NgramSalienceCalculator():\n",
        "    def __init__(self, old_corpus, modern_corpus, use_ngrams=False):\n",
        "        ngrams = (1, 3) if use_ngrams else (1, 1)\n",
        "        self.vectorizer = CountVectorizer(ngram_range=ngrams)\n",
        "\n",
        "        old_count_matrix = self.vectorizer.fit_transform(old_corpus)\n",
        "        self.old_vocab = self.vectorizer.vocabulary_\n",
        "        self.old_counts = np.sum(old_count_matrix, axis=0)\n",
        "\n",
        "        modern_count_matrix = self.vectorizer.fit_transform(modern_corpus)\n",
        "        self.modern_vocab = self.vectorizer.vocabulary_\n",
        "        self.modern_counts = np.sum(modern_count_matrix, axis=0)\n",
        "\n",
        "    def salience(self, feature, attribute='old', lmbda=0.5):\n",
        "        assert attribute in ['old', 'modern']\n",
        "        if feature not in self.old_vocab:\n",
        "            old_count = 0.0\n",
        "        else:\n",
        "            old_count = self.old_counts[0, self.old_vocab[feature]]\n",
        "\n",
        "        if feature not in self.modern_vocab:\n",
        "            modern_count = 0.0\n",
        "        else:\n",
        "            modern_count = self.modern_counts[0, self.modern_vocab[feature]]\n",
        "\n",
        "        if attribute == 'old':\n",
        "            return (old_count + lmbda) / (modern_count + lmbda)\n",
        "        else:\n",
        "            return (modern_count + lmbda) / (old_count + lmbda)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "4LymTZT63gMa",
        "outputId": "f4062115-8804-4833-a3fc-e30767430721",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28434\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "c = Counter()\n",
        "\n",
        "for corpus in [old_corpus_path, modern_corpus_path]:\n",
        "  for sent in corpus:\n",
        "    for tok in sent.strip().split():\n",
        "      c[tok] += 1\n",
        "\n",
        "print(len(c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "yYB0FMsk3gMa",
        "outputId": "3cccc90d-30f0-415d-e2c6-42a2505d211f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28434\n"
          ]
        }
      ],
      "source": [
        "vocab = {w for w, _ in c.most_common() if _ > 0}  # if we took words with > 1 occurences, vocabulary would be x2 smaller, but we'll survive this size\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_old = [' '.join([w if w in vocab else '<unk>' for w in sent.strip().split()]) for sent in old_corpus_path]\n",
        "corpus_modern = [' '.join([w if w in vocab else '<unk>' for w in sent.strip().split()]) for sent in modern_corpus_path]"
      ],
      "metadata": {
        "id": "9a3R4EawYzvs"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "uxKRQKFn3gMb"
      },
      "outputs": [],
      "source": [
        "old_out_name = 'old-words.txt'\n",
        "modern_out_name = 'modern-words.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "9kssF7VP3gMb"
      },
      "outputs": [],
      "source": [
        "threshold = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "TBMHG8fI3gMb"
      },
      "outputs": [],
      "source": [
        "sc = NgramSalienceCalculator(corpus_old, corpus_modern, False)\n",
        "seen_grams = set()\n",
        "\n",
        "with open(old_out_name, 'w') as old_out, open(modern_out_name, 'w') as modern_out:\n",
        "    for gram in set(sc.old_vocab.keys()).union(set(sc.modern_vocab.keys())):\n",
        "        if gram not in seen_grams:\n",
        "            seen_grams.add(gram)\n",
        "            old_salience = sc.salience(gram, attribute='old')\n",
        "            modern_salience = sc.salience(gram, attribute='modern')\n",
        "            if old_salience > threshold:\n",
        "                old_out.writelines(f'{gram}\\n')\n",
        "            elif modern_salience > threshold:\n",
        "                modern_out.writelines(f'{gram}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0KGimiH3gMc"
      },
      "source": [
        "## 2.2 Evaluating word shakespearities with a logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "LESNB5x03gMc"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "pipe = make_pipeline(CountVectorizer(), LogisticRegression(max_iter=1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "pB1DKUNk3gMc"
      },
      "outputs": [],
      "source": [
        "X_train = corpus_old + corpus_modern\n",
        "y_train = [1] * len(corpus_old) + [0] * len(corpus_modern)\n",
        "pipe.fit(X_train, y_train);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "GGZngwx13gMc",
        "outputId": "fcf1565c-43cf-4bc5-8fcc-84971d7ff4eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23329,)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "coefs = pipe[1].coef_[0]\n",
        "coefs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "vTu5GIZk3gMc"
      },
      "outputs": [],
      "source": [
        "word2coef = {w: coefs[idx] for w, idx in pipe[0].vocabulary_.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "8dAh4oZJ3gMc"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('word2coef.pkl', 'wb') as f:\n",
        "    pickle.dump(word2coef, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnhq5Vtw3gMd"
      },
      "source": [
        "## 2.3 Labelling BERT tokens by shakespearity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "EBl4CDY93gMd",
        "outputId": "2b4cfa79-0c6e-4fd1-e66f-21c30ec8ca1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20000/20000 [00:10<00:00, 1964.04it/s]\n",
            "100%|██████████| 20000/20000 [00:11<00:00, 1748.21it/s]\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "old_counter = defaultdict(lambda: 1)\n",
        "modern_counter = defaultdict(lambda: 1)\n",
        "\n",
        "for text in tqdm(corpus_old):\n",
        "    for token in tokenizer.encode(text):\n",
        "        old_counter[token] += 1\n",
        "for text in tqdm(corpus_modern):\n",
        "    for token in tokenizer.encode(text):\n",
        "        modern_counter[token] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "7nQ-nwYh3gMd"
      },
      "outputs": [],
      "source": [
        "token_shakespearean = [old_counter[i] / (modern_counter[i] + old_counter[i]) for i in range(len(tokenizer.vocab))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "W2bh-1p63gMd"
      },
      "outputs": [],
      "source": [
        "with open('token_shakespearean.txt', 'w') as f:\n",
        "    for t in token_shakespearean:\n",
        "        f.write(str(t))\n",
        "        f.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn5WIP7H3gMd"
      },
      "source": [
        "# 3. Setting up the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6IjI_TS3gMe"
      },
      "source": [
        "### 3.1 Loading the vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "JjqzUEg-3gMe"
      },
      "outputs": [],
      "source": [
        "with open('old-words.txt', 'r') as f:\n",
        "    s = f.readlines()\n",
        "old_words = list(map(lambda x: x[:-1], s))\n",
        "\n",
        "with open('modern-words.txt', 'r') as f:\n",
        "    s = f.readlines()\n",
        "modern_words = list(map(lambda x: x[:-1], s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "mTF-BKBT3gMe"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('word2coef.pkl', 'rb') as f:\n",
        "    word2coef = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "eE580z4w3gMf"
      },
      "outputs": [],
      "source": [
        "token_shakespearean = []\n",
        "with open('token_shakespearean.txt', 'r') as f:\n",
        "    for line in f.readlines():\n",
        "        token_shakespearean.append(float(line))\n",
        "token_shakespearean = np.array(token_shakespearean)\n",
        "token_shakespearean = np.maximum(0, np.log(1/(1/token_shakespearean-1)))   # log odds ratio\n",
        "\n",
        "# discourage meaningless tokens\n",
        "for tok in ['.', ',', '-', ';']:\n",
        "    token_shakespearean[tokenizer.encode(tok)][1] = 3\n",
        "\n",
        "for tok in ['you']:\n",
        "    token_shakespearean[tokenizer.encode(tok)][1] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying the model"
      ],
      "metadata": {
        "id": "kmFuRdmgKMuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reload(condbert)\n",
        "from condbert import CondBertRewriter\n",
        "\n",
        "editor = CondBertRewriter(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    neg_words=old_words,\n",
        "    pos_words=modern_words,\n",
        "    word2coef=word2coef,\n",
        "    token_toxicities=token_shakespearean,\n",
        "    predictor=None\n",
        ")"
      ],
      "metadata": {
        "id": "sF_4F6tnKUUt"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(editor.translate('Thou canst not fear us , Pompey , with thy sails .', prnt=False))"
      ],
      "metadata": {
        "id": "ivI7Ms81Kq-N",
        "outputId": "ee8c7064-dc69-44a8-d78b-54cab4b7f693",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you cant not fear us , pttai , with your sails .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_preprocessing"
      ],
      "metadata": {
        "id": "0BOJaKw1Mgg8",
        "outputId": "e41b8ba1-83e8-4134-84f5-4412c47463f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras_preprocessing\n",
            "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from keras_preprocessing) (1.22.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from keras_preprocessing) (1.16.0)\n",
            "Installing collected packages: keras_preprocessing\n",
            "Successfully installed keras_preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from multiword import masked_token_predictor_bert\n",
        "reload(masked_token_predictor_bert)\n",
        "from multiword.masked_token_predictor_bert import MaskedTokenPredictorBert"
      ],
      "metadata": {
        "id": "1DtOyCttMa7-"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = MaskedTokenPredictorBert(model, tokenizer, max_len=250, device=device, label=0, contrast_penalty=0.0)\n",
        "editor.predictor = predictor\n",
        "\n",
        "def adjust_logits(logits, label):\n",
        "    return logits - editor.token_toxicities * 3\n",
        "\n",
        "predictor.logits_postprocessor = adjust_logits\n",
        "\n",
        "print(editor.replacement_loop('Thou canst not fear us , Pompey , with thy sails .', verbose=False))"
      ],
      "metadata": {
        "id": "TSVhS9QPMvOY",
        "outputId": "995279c6-1ea0-4b6d-fdce-2c603f6669f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you can not fear us , po , with your sails .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(editor.replacement_loop('Thou canst not fear us , Pompey , with thy sails .', verbose=False, n_units=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WpZFGUMeyql",
        "outputId": "17acb6b2-0b03-4f4f-860b-c5c338e5b907"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you can not fear us , po , with your sails .\n",
            "CPU times: user 2.43 s, sys: 3.25 ms, total: 2.43 s\n",
            "Wall time: 2.64 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "print(editor.replacement_loop('Thou canst not fear us , Pompey , with thy sails .', verbose=False, n_units=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbeFHZesezl8",
        "outputId": "a08ffaad-30ee-4f9e-9875-2336f0ea17ac"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
            "Wall time: 7.87 µs\n",
            "you filth not fear us , po , with your sails .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(editor.replacement_loop('Thou canst not fear us , Pompey , with thy sails .', verbose=False, n_units=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4G4ipvWe1yg",
        "outputId": "72c75ae6-7880-4836-83ae-df05488f0209"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you filth not fear us , po , with your sails .\n",
            "CPU times: user 8.98 s, sys: 0 ns, total: 8.98 s\n",
            "Wall time: 11.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flair==0.11"
      ],
      "metadata": {
        "id": "BKdMwQhoNQ3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "dhZvhGK03gMh"
      },
      "outputs": [],
      "source": [
        "import choosers\n",
        "reload(choosers)\n",
        "from choosers import EmbeddingSimilarityChooser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = MaskedTokenPredictorBert(\n",
        "    model, tokenizer, max_len=250, device=device, label=0, contrast_penalty=0.0, \n",
        "    confuse_bert_args=True, # this argument deteriorates quality but is used for backward compatibility\n",
        ")\n",
        "editor.predictor = predictor\n",
        "\n",
        "def adjust_logits(logits, label=0):\n",
        "    return logits - editor.token_toxicities * 10\n",
        "\n",
        "predictor.logits_postprocessor = adjust_logits\n",
        "\n",
        "cho = EmbeddingSimilarityChooser(sim_coef=100, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "f2g1GFfwfxtJ"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "old_test_ds = dataset['og'][40000:].to_list()"
      ],
      "metadata": {
        "id": "R26fvOGCOKnG"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sent in enumerate(tqdm(old_test_ds[10:15])):\n",
        "  inp = sent.strip()\n",
        "  out = editor.replacement_loop(inp, verbose=False, chooser=cho, n_top=10, n_tokens=(1,2,3), n_units=1)\n",
        "  print('\\n', inp, '\\n', out)"
      ],
      "metadata": {
        "id": "E0ffh0TX8f8t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "655f8556-33b3-4110-b3ea-9aab4920e8ea"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [00:00<00:02,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " i serve you , madam . your graces are right welcome . \n",
            " i serve you , madam . your \" \" \" are right welcome .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [00:02<00:04,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " good dawning to thee , friend . art of this house ? \n",
            " good the to \" \" \" , friend . \" . of this house ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [00:03<00:02,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ay . \n",
            " . \" . .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [00:05<00:01,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " where may we set our horses ? \n",
            " where they we they our horses ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:06<00:00,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " i th mire . \n",
            " i i \" \" \" .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Переведем с помощью conbert 1000 предложений для дальнейшей оценки качества и сохраним ее в файл model_outputs.txt"
      ],
      "metadata": {
        "id": "qPAUd-t-nvcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "SfpcrQjU2QS8"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test1000 = random.sample(old_test_ds, 1000)"
      ],
      "metadata": {
        "id": "h__CzJYK2M7M"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_outputs = []\n",
        "for i, sent in enumerate(tqdm(test1000)):\n",
        "  inp = sent.strip()\n",
        "  out = editor.replacement_loop(inp, verbose=False, chooser=cho, n_top=10, n_tokens=(1,2,3), n_units=1)\n",
        "  model_outputs.append(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nXuJIZ3oWAy",
        "outputId": "9444ed03-7add-4a8f-c09b-aaf5ad18bf9d"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [27:21<00:00,  1.64s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('model_outputs.txt', 'w') as f:\n",
        "    for sent in model_outputs:\n",
        "      f.writelines(f'{sent}\\n')"
      ],
      "metadata": {
        "id": "ZM01DCKkTE8k"
      },
      "execution_count": 160,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}