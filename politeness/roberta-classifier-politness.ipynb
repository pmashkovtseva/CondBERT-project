{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install evaluate --quiet","metadata":{"execution":{"iopub.status.busy":"2023-03-24T10:54:55.992865Z","iopub.execute_input":"2023-03-24T10:54:55.993175Z","iopub.status.idle":"2023-03-24T10:55:09.836988Z","shell.execute_reply.started":"2023-03-24T10:54:55.993146Z","shell.execute_reply":"2023-03-24T10:55:09.835781Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom sklearn.model_selection import train_test_split\nimport evaluate\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-03-24T11:00:13.470933Z","iopub.execute_input":"2023-03-24T11:00:13.471315Z","iopub.status.idle":"2023-03-24T11:00:15.357113Z","shell.execute_reply.started":"2023-03-24T11:00:13.471281Z","shell.execute_reply":"2023-03-24T11:00:15.356127Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df_p = pd.read_csv('/kaggle/input/politenessdataset/politeness.csv')\ndf_p = df_p[df_p['is_useful']==1]\npolite = df_p[df_p['score'] > 0.6][:60000]\nnonpolite = df_p[df_p['score'] < 0.4][:60000]\n# print(len(polite))\n# print(len(nonpolite))","metadata":{"execution":{"iopub.status.busy":"2023-03-24T10:58:04.483652Z","iopub.execute_input":"2023-03-24T10:58:04.484041Z","iopub.status.idle":"2023-03-24T10:58:09.423425Z","shell.execute_reply.started":"2023-03-24T10:58:04.484007Z","shell.execute_reply":"2023-03-24T10:58:09.422196Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"60000\n60000\n","output_type":"stream"}]},{"cell_type":"code","source":"un_df = pd.concat([polite, nonpolite])['txt'].tolist()\nlabels = [[0.0, 1.0]] * len(polite) + [[1.0, 0.0]] * len(nonpolite)\n\nX_train, X_test, y_train, y_test = train_test_split(un_df, labels, test_size=0.33, random_state=42, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-24T10:59:14.874016Z","iopub.execute_input":"2023-03-24T10:59:14.874694Z","iopub.status.idle":"2023-03-24T10:59:14.960555Z","shell.execute_reply.started":"2023-03-24T10:59:14.874656Z","shell.execute_reply":"2023-03-24T10:59:14.959420Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('roberta-base')\nmodel = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels = 2)","metadata":{"execution":{"iopub.status.busy":"2023-03-24T10:59:30.202541Z","iopub.execute_input":"2023-03-24T10:59:30.203127Z","iopub.status.idle":"2023-03-24T10:59:39.212008Z","shell.execute_reply.started":"2023-03-24T10:59:30.203089Z","shell.execute_reply":"2023-03-24T10:59:39.211064Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1217e729bc34dedb2bfff3ec1173093"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d491cd46f5d94b0aa0d812671e0cf6f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a0437fa90924d07ad0e1278fa427da5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ae6f147c36e4aa8ac12b89ec0d23abe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f02b1c945954525b6a7ec5ba94490ce"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"def prep(text, tokenizer=tokenizer):\n    return tokenizer(text, padding = 'max_length', max_length = 128, truncation=True, return_tensors='pt')","metadata":{"execution":{"iopub.status.busy":"2023-03-24T10:59:42.861414Z","iopub.execute_input":"2023-03-24T10:59:42.862105Z","iopub.status.idle":"2023-03-24T10:59:42.870905Z","shell.execute_reply.started":"2023-03-24T10:59:42.862067Z","shell.execute_reply":"2023-03-24T10:59:42.866173Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"X_train = prep(X_train)\nX_test = prep(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-03-24T10:59:47.336437Z","iopub.execute_input":"2023-03-24T10:59:47.336894Z","iopub.status.idle":"2023-03-24T10:59:59.730007Z","shell.execute_reply.started":"2023-03-24T10:59:47.336853Z","shell.execute_reply":"2023-03-24T10:59:59.728946Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class TextDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = torch.tensor(labels)\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = TextDataset(X_train, y_train)\nval_dataset = TextDataset(X_test, y_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-24T11:00:04.888778Z","iopub.execute_input":"2023-03-24T11:00:04.889382Z","iopub.status.idle":"2023-03-24T11:00:04.920949Z","shell.execute_reply.started":"2023-03-24T11:00:04.889330Z","shell.execute_reply":"2023-03-24T11:00:04.919774Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"metric = evaluate.load(\"accuracy\")","metadata":{"execution":{"iopub.status.busy":"2023-03-24T11:00:20.095666Z","iopub.execute_input":"2023-03-24T11:00:20.096121Z","iopub.status.idle":"2023-03-24T11:00:21.001876Z","shell.execute_reply.started":"2023-03-24T11:00:20.096079Z","shell.execute_reply":"2023-03-24T11:00:21.000797Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98c740e48be1414593e0a1c90759252e"}},"metadata":{}}]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=[ np.argmax(np.asarray(i)) for i in labels])","metadata":{"execution":{"iopub.status.busy":"2023-03-24T11:22:24.495443Z","iopub.execute_input":"2023-03-24T11:22:24.496478Z","iopub.status.idle":"2023-03-24T11:22:24.508846Z","shell.execute_reply.started":"2023-03-24T11:22:24.496415Z","shell.execute_reply":"2023-03-24T11:22:24.506215Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(output_dir=\"./politeness_clf_roberta\",\n                                  evaluation_strategy=\"epoch\",\n                                  per_device_train_batch_size = 32,\n                                  per_device_eval_batch_size = 32,\n                                  save_strategy = 'epoch',\n                                  num_train_epochs=2,\n                                  save_total_limit =1)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-24T11:22:30.397202Z","iopub.execute_input":"2023-03-24T11:22:30.397888Z","iopub.status.idle":"2023-03-24T11:22:30.426082Z","shell.execute_reply.started":"2023-03-24T11:22:30.397849Z","shell.execute_reply":"2023-03-24T11:22:30.424761Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret('wandb-key') \nwandb.login(key=wandb_api)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-03-24T11:22:41.739290Z","iopub.execute_input":"2023-03-24T11:22:41.739830Z","iopub.status.idle":"2023-03-24T11:59:28.264865Z","shell.execute_reply.started":"2023-03-24T11:22:41.739795Z","shell.execute_reply":"2023-03-24T11:59:28.263518Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 80400\n  Num Epochs = 2\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 5026\n  Number of trainable parameters = 124647170\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  import sys\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5026' max='5026' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5026/5026 36:45, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.213400</td>\n      <td>0.215993</td>\n      <td>0.929369</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.164600</td>\n      <td>0.174078</td>\n      <td>0.946263</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 39600\n  Batch size = 32\nSaving model checkpoint to ./politeness_clf_roberta/checkpoint-2513\nConfiguration saved in ./politeness_clf_roberta/checkpoint-2513/config.json\nModel weights saved in ./politeness_clf_roberta/checkpoint-2513/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  import sys\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  \n***** Running Evaluation *****\n  Num examples = 39600\n  Batch size = 32\nSaving model checkpoint to ./politeness_clf_roberta/checkpoint-5026\nConfiguration saved in ./politeness_clf_roberta/checkpoint-5026/config.json\nModel weights saved in ./politeness_clf_roberta/checkpoint-5026/pytorch_model.bin\nDeleting older checkpoint [politeness_clf_roberta/checkpoint-2513] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5026, training_loss=0.21800658259788358, metrics={'train_runtime': 2206.2378, 'train_samples_per_second': 72.884, 'train_steps_per_second': 2.278, 'total_flos': 1.0577064425472e+16, 'train_loss': 0.21800658259788358, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.save_pretrained('roberta_polit_clf')\ntrainer.model.save_pretrained('roberta_polit_clf')","metadata":{"execution":{"iopub.status.busy":"2023-03-24T12:02:30.908895Z","iopub.execute_input":"2023-03-24T12:02:30.909822Z","iopub.status.idle":"2023-03-24T12:02:31.822950Z","shell.execute_reply.started":"2023-03-24T12:02:30.909769Z","shell.execute_reply":"2023-03-24T12:02:31.821723Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"tokenizer config file saved in roberta_polit_clf/tokenizer_config.json\nSpecial tokens file saved in roberta_polit_clf/special_tokens_map.json\nConfiguration saved in roberta_polit_clf/config.json\nModel weights saved in roberta_polit_clf/pytorch_model.bin\n","output_type":"stream"}]}]}
